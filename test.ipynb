{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5137f06-8336-4340-b37e-51eda2edd4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory:  /home/peizhi/Documents/flame-head-tracker\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "## Enviroment Setup\n",
    "import os, sys\n",
    "DIR = os.getcwd()\n",
    "WORKING_DIR = '/home/peizhi/Documents/flame-head-tracker'\n",
    "os.chdir(WORKING_DIR) # change the working directory to the project's absolute path\n",
    "print(\"Current Working Directory: \", os.getcwd())\n",
    "\n",
    "## Computing Device\n",
    "device = 'cuda:0'\n",
    "import torch\n",
    "torch.cuda.set_device(device) # this will solve the problem that OpenGL not on the same device with torch tensors\n",
    "\n",
    "sys.path.append(WORKING_DIR)\n",
    "sys.path.append('./utils/flame_lib/')\n",
    "sys.path.append('./utils/flame_fitting/')\n",
    "sys.path.append('./utils/face_parsing/')\n",
    "sys.path.append('./utils/decalib/')\n",
    "sys.path.append('./utils/mesh_renderer')\n",
    "sys.path.append('./utils/scene')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from tracker_base import Tracker\n",
    "\n",
    "from utils.graphics_utils import create_diff_world_to_view_matrix, verts_clip_to_ndc\n",
    "from utils.scene.cameras import PerspectiveCamera\n",
    "from utils.mesh_renderer import NVDiffRenderer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a59dc29-3e26-4ea0-8841-e7d470313125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731020728.459138 2063686 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1731020728.499376 2063959 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.120), renderer: NVIDIA RTX A6000/PCIe/SSE2\n",
      "W0000 00:00:1731020728.500042 2063686 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1731020728.509773 2063966 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731020728.527013 2063978 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peizhi/Documents/flame-head-tracker/utils/flame_lib/FLAME.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('dynamic_lmk_faces_idx', torch.tensor(lmk_embeddings['dynamic_lmk_faces_idx'], dtype=torch.long))\n",
      "/home/peizhi/Documents/flame-head-tracker/utils/flame_lib/FLAME.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('dynamic_lmk_bary_coords', torch.tensor(lmk_embeddings['dynamic_lmk_bary_coords'], dtype=self.dtype))\n",
      "/home/peizhi/miniconda3/envs/tracker/lib/python3.10/site-packages/pytorch3d/io/obj_io.py:550: UserWarning: Mtl file does not exist: ./models/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n",
      "/home/peizhi/miniconda3/envs/tracker/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/peizhi/miniconda3/envs/tracker/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n",
      "trained model found. load models/deca_model.tar\n",
      "Flame Tracker ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peizhi/miniconda3/envs/tracker/lib/python3.10/site-packages/pytorch3d/io/obj_io.py:550: UserWarning: Mtl file does not exist: models/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################\n",
    "## Setup Flame Tracker    #     \n",
    "###########################\n",
    "\n",
    "tracker_cfg = {\n",
    "    'mediapipe_face_landmarker_v2_path': './models/face_landmarker_v2_with_blendshapes.task',\n",
    "    'flame_model_path': './models/FLAME2020/generic_model.pkl',\n",
    "    'flame_lmk_embedding_path': './models/landmark_embedding.npy',\n",
    "    'tex_space_path': './models/FLAME_texture.npz',\n",
    "    'face_parsing_model_path': './utils/face_parsing/79999_iter.pth',\n",
    "    'uv_coord_mapping_file_path': './models/uv2vert_256.npy',\n",
    "    'template_mesh_file_path': './models/head_template.obj',\n",
    "    'result_img_size': 512,\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "tracker = Tracker(tracker_cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97a44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def render_flame_shape(shape, exp, pose, eye_pose):\n",
    "\n",
    "    cam_pose = np.array([0,0,0,0,0,0.8], dtype=np.float32)\n",
    "\n",
    "    # Prepare Tensors\n",
    "    shape = torch.from_numpy(shape).to(device)\n",
    "    exp = torch.from_numpy(exp).to(device)\n",
    "    pose = torch.from_numpy(pose).to(device)\n",
    "    eye_pose = torch.from_numpy(eye_pose).to(device)\n",
    "    cam_pose = torch.from_numpy(cam_pose).to(device)\n",
    "\n",
    "    # FLAME Reconstruction\n",
    "    vertices, _, _ = tracker.flame(shape_params=shape, expression_params=exp, pose_params=pose, eye_pose_params=eye_pose)\n",
    "\n",
    "    # Prepare Camera\n",
    "    Rt = create_diff_world_to_view_matrix(cam_pose)\n",
    "    cam = PerspectiveCamera(Rt=Rt, fov=tracker.fov, bg=tracker.bg_color, \n",
    "                            image_width=tracker.W, image_height=tracker.H, znear=tracker.znear, zfar=tracker.zfar)\n",
    "\n",
    "    # Render\n",
    "    new_mesh_renderer = NVDiffRenderer().to(tracker.device) # there seems to be a bug with the NVDiffRenderer, so I create this new\n",
    "                                                            # render everytime to render the image\n",
    "    rendered = new_mesh_renderer.render_from_camera(vertices, tracker.mesh_faces, cam) # vertices should have the shape of [1, N, 3]\n",
    "    verts_clip = rendered['verts_clip'] # [1, N, 3]\n",
    "    verts_ndc_3d = verts_clip_to_ndc(verts_clip, image_size=tracker.H, out_dim=3) # convert the clipped vertices to NDC, output [N, 3]\n",
    "    landmarks3d = tracker.flame.seletec_3d68(verts_ndc_3d[None]) # [1, 68, 3]\n",
    "    landmarks2d = landmarks3d[:,:,:2].detach().cpu().numpy() # [1, 68, 2]\n",
    "    eyes_landmarks3d = verts_ndc_3d[tracker.R_EYE_INDICES + tracker.L_EYE_INDICES][None]  # [1, 10, 3]\n",
    "    eyes_landmarks2d = eyes_landmarks3d[:,:,:2].detach().cpu().numpy()  # [1, 10, 2]\n",
    "    rendered_mesh_shape = rendered['rgba'][0,...,:3].detach().cpu().numpy()\n",
    "    rendered_mesh_shape = np.array(np.clip(rendered_mesh_shape * 255, 0, 255), dtype=np.uint8) # uint8\n",
    "\n",
    "    # Draw 2D landmarks as green dots\n",
    "    for coords in landmarks2d[0]:\n",
    "        coords = np.clip(coords, 0, tracker.H).astype(np.uint8)\n",
    "        #coords = np.clip((coords / 2 + 1) * self.H, 0, self.H).astype(np.uint8)\n",
    "        cv2.circle(rendered_mesh_shape, (coords[0], coords[1]), radius=1, color=(0, 255, 0), thickness=-1)  # Green color, filled circle\n",
    "\n",
    "    # Optionally draw eye landmarks as red dots\n",
    "    for coords in eyes_landmarks2d[0]:\n",
    "        coords = np.clip(coords, 0, tracker.H).astype(np.uint8)\n",
    "        #coords = np.clip((coords / 2 + 1) * self.H, 0, self.H).astype(np.uint8)\n",
    "        cv2.circle(rendered_mesh_shape, (coords[0], coords[1]), radius=1, color=(0, 0, 255), thickness=-1)  # Red color, filled circle\n",
    "\n",
    "    return rendered_mesh_shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fcce9532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blendshape to Expression mapping shape: (52, 50)\n",
      "Blendshape to Pose mapping shape: (52, 6)\n",
      "Blendshape to Eye Pose mapping shape: (52, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load the mapping arrays from the specified directory using os.path.join\n",
    "bs2exp = np.load(os.path.join(DIR, 'mappings/bs2exp.npy'))\n",
    "bs2pose = np.load(os.path.join(DIR, 'mappings/bs2pose.npy'))\n",
    "bs2eye = np.load(os.path.join(DIR, 'mappings/bs2eye.npy'))\n",
    "\n",
    "# # Function to normalize a matrix with min-max scaling to range [0, 1]\n",
    "# def minmax_normalize(matrix):\n",
    "#     min_val = matrix.min()\n",
    "#     max_val = matrix.max()\n",
    "#     return (matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "# # Normalize each matrix\n",
    "# bs2exp = minmax_normalize(bs2exp)\n",
    "# bs2pose = minmax_normalize(bs2pose)\n",
    "# bs2eye = minmax_normalize(bs2eye)\n",
    "\n",
    "# Check if the mappings loaded correctly (optional)\n",
    "print(\"Blendshape to Expression mapping shape:\", bs2exp.shape)\n",
    "print(\"Blendshape to Pose mapping shape:\", bs2pose.shape)\n",
    "print(\"Blendshape to Eye Pose mapping shape:\", bs2eye.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170855a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e9d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d41fcb3b4a4b3e8dffb24e433a8ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Blendshape', max=52, min=1), FloatSlider(value=0.0, descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_render(selected_blendshape, strength)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Initialize blendshape and other arrays\n",
    "shape = np.zeros([1, 100], dtype=np.float32)\n",
    "exp = np.zeros([1, 50], dtype=np.float32)\n",
    "pose = np.zeros([1, 6], dtype=np.float32)\n",
    "eye_pose = np.zeros([1, 6], dtype=np.float32)\n",
    "\n",
    "# Initialize blendshape scores\n",
    "blendshape_scores = np.zeros([1, 52], dtype=np.float32)\n",
    "\n",
    "# Define a function to render and update the FLAME shape\n",
    "def update_render(selected_blendshape, strength):\n",
    "    # Reset blendshape scores\n",
    "    blendshape_scores.fill(0)\n",
    "    \n",
    "    # Set the selected blendshape to the specified strength\n",
    "    blendshape_scores[0, selected_blendshape-1] = strength\n",
    "\n",
    "    # Calculate expression, pose, and eye_pose using the mappings\n",
    "    exp = blendshape_scores @ bs2exp #*1e-8\n",
    "    pose = blendshape_scores @ bs2pose #* 1e-8\n",
    "    pose[0, :3] = 0  # Resetting head rotation as in your example\n",
    "    eye_pose = blendshape_scores @ bs2eye #* 1e-5\n",
    "\n",
    "    # Clear previous plot\n",
    "    plt.clf()\n",
    "\n",
    "    # Render the updated FLAME shape\n",
    "    plt.imshow(render_flame_shape(shape, exp, pose, eye_pose))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Creating sliders for blendshape index and strength\n",
    "blendshape_slider = IntSlider(value=1, min=1, max=52, step=1, description=\"Blendshape\")\n",
    "strength_slider = FloatSlider(value=0.0, min=0.0, max=1.0, step=0.01, description=\"Strength\")\n",
    "\n",
    "# Using interact to link sliders to the rendering function\n",
    "interact(update_render, selected_blendshape=blendshape_slider, strength=strength_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a17b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
